# üöÄ Scaling a 200 Utenti Simultanei - Guida Completa

## üéØ Obiettivo
Configurare il sistema per gestire **200 utenti che inviano messaggi contemporaneamente** senza errori.

---

## üìä Calcoli Necessari

### Formula Capacit√† Utenti
```
Capacit√† = MAX_CONCURRENT + MAX_QUEUE_SIZE

Esempio attuale:
5 (processing) + 100 (queue) = 105 utenti max
```

### Formula Throughput OpenAI
```
Throughput = MAX_CONCURRENT √ó (1000ms / MIN_REQUEST_DELAY) √ó 60

Esempio con MAX_CONCURRENT=5, DELAY=200ms:
5 √ó (1000/200) √ó 60 = 5 √ó 5 √ó 60 = 1,500 req/min
```

---

## üîß Configurazioni per Tier OpenAI

### **Tier 1 (500 RPM)** - ‚ùå NON sufficiente per 200 utenti

**Limiti**: 500 requests/min
**Capacit√† max**: ~50-100 utenti con attesa in coda
**Problema**: Con 200 utenti, 100+ verranno rifiutati

**Configurazione ottimale** (gi√† implementata):
```typescript
// app/lib/openai-queue.ts
private readonly MAX_CONCURRENT = 5;
private readonly MAX_QUEUE_SIZE = 100;
private readonly MIN_REQUEST_DELAY = 200;
```

**Throughput**: 1,500 req/min (MA OpenAI accetta solo 500 RPM)
**Utenti simultanei**: ~105 (poi sovraccarico)

**Soluzione**: Upgrade a Tier 2 o 3

---

### **Tier 2 (5,000 RPM)** - ‚ö†Ô∏è Borderline per 200 utenti

**Limiti**: 5,000 requests/min
**Capacit√† reale**: ~150-180 utenti con buona gestione coda

**Configurazione consigliata**:
```typescript
// app/lib/openai-queue.ts
private readonly MAX_CONCURRENT = 12;
private readonly MAX_QUEUE_SIZE = 200;
private readonly MIN_REQUEST_DELAY = 100;
```

**Throughput**: 12 √ó (1000/100) √ó 60 = 7,200 req/min
**MA**: Con retry e variabilit√†, mediamente ~4,000 req/min ‚Üí OK per Tier 2

**Utenti simultanei**: 12 + 200 = **212 utenti** ‚úÖ

**Nota**: Funziona se non tutti i 200 utenti inviano messaggi ogni secondo.

---

### **Tier 3+ (10,000 RPM)** - ‚úÖ IDEALE per 200+ utenti

**Limiti**: 10,000 requests/min
**Capacit√† reale**: 200-300+ utenti con margine di sicurezza

**Configurazione consigliata**:
```typescript
// app/lib/openai-queue.ts
private readonly MAX_CONCURRENT = 20;
private readonly MAX_QUEUE_SIZE = 250;
private readonly MIN_REQUEST_DELAY = 50;
```

**Throughput**: 20 √ó (1000/50) √ó 60 = 24,000 req/min burst
**Mediamente**: ~8,000 req/min sostenuto ‚Üí OK per Tier 3

**Utenti simultanei**: 20 + 250 = **270 utenti** ‚úÖ

**Vantaggio**: Margine del 35% per gestire spike

---

### **Tier 4+ (10,000+ RPM)** - üöÄ Enterprise

**Limiti**: 10,000+ requests/min (custom)
**Capacit√†**: Illimitata praticamente

**Configurazione aggressiva**:
```typescript
private readonly MAX_CONCURRENT = 30;
private readonly MAX_QUEUE_SIZE = 300;
private readonly MIN_REQUEST_DELAY = 30;
```

**Utenti simultanei**: 330+ ‚úÖ

---

## üõ†Ô∏è Come Applicare le Modifiche

### 1. **Verifica il Tuo Tier OpenAI**
```bash
# Vai su OpenAI Dashboard
https://platform.openai.com/settings/organization/limits

# Oppure via API
curl https://api.openai.com/v1/models \
  -H "Authorization: Bearer $OPENAI_API_KEY"
```

### 2. **Scegli la Configurazione Appropriata**

Modifica `app/lib/openai-queue.ts` in base al tuo tier (vedi tabelle sopra).

### 3. **Esempio: Configurazione per Tier 3 (10K RPM)**

```typescript
// app/lib/openai-queue.ts (linee 38-47)

class OpenAIQueue {
  private queue: QueueItem<any>[] = [];
  private processing = 0;
  private lastRequestTime = 0;

  // ‚öôÔ∏è Configurazione per OpenAI Tier 3+ (10K RPM, 200+ utenti)
  private readonly MAX_CONCURRENT = 20;    // ‚Üê Aumentato da 5
  private readonly MAX_QUEUE_SIZE = 250;   // ‚Üê Aumentato da 100
  private readonly REQUEST_TIMEOUT = 15000;
  private readonly MIN_REQUEST_DELAY = 50; // ‚Üê Ridotto da 200ms
  private readonly MAX_RETRIES = 3;
  private readonly INITIAL_RETRY_DELAY = 1000;
}
```

### 4. **Commit e Deploy**

```bash
git add app/lib/openai-queue.ts
git commit -m "scale: configure for Tier 3 - 200+ concurrent users"
git push origin main

# Vercel deploier√† automaticamente
```

### 5. **Verifica Deploy**

```bash
# Attendi 2-3 minuti, poi verifica
curl https://your-app.vercel.app/api/health | jq

# Output atteso:
{
  "queue": {
    "maxConcurrent": 20,   # ‚Üê Dovrebbe essere 20 ora
    "maxQueueSize": 250,   # ‚Üê Dovrebbe essere 250
    "requestDelay": 50     # ‚Üê Dovrebbe essere 50
  }
}
```

---

## üìä Tabella Riepilogativa

| Tier | RPM | MAX_CONCURRENT | QUEUE_SIZE | DELAY | Capacit√† | Costo/mese |
|------|-----|----------------|------------|-------|----------|------------|
| Tier 1 | 500 | 5 | 100 | 200ms | 105 utenti | $5+ |
| Tier 2 | 5K | 12 | 200 | 100ms | 212 utenti | $50+ |
| **Tier 3** | **10K** | **20** | **250** | **50ms** | **270 utenti** ‚úÖ | **$100+** |
| Tier 4+ | 10K+ | 30 | 300 | 30ms | 330+ utenti | $250+ |

---

## üß™ Test di Carico

### Script per Simulare 200 Utenti

```bash
#!/bin/bash
# test-200-users.sh

ENDPOINT="https://your-app.vercel.app/api/assistants/threads"
SUCCESS=0
FAILED=0

echo "Creating 200 threads simultaneously..."

for i in {1..200}; do
  (
    RESPONSE=$(curl -s -w "%{http_code}" -X POST "$ENDPOINT" -H "Content-Type: application/json")
    HTTP_CODE=$(echo "$RESPONSE" | tail -c 4)

    if [ "$HTTP_CODE" = "200" ]; then
      echo "[$i] ‚úÖ Success"
      ((SUCCESS++))
    else
      echo "[$i] ‚ùå Failed (HTTP $HTTP_CODE)"
      ((FAILED++))
    fi
  ) &
done

wait

echo ""
echo "Results:"
echo "‚úÖ Success: $SUCCESS"
echo "‚ùå Failed: $FAILED"
echo "Success rate: $(echo "scale=2; $SUCCESS * 100 / 200" | bc)%"
```

**Come usare**:
```bash
chmod +x test-200-users.sh
./test-200-users.sh
```

**Risultati attesi**:
- **Tier 1** (5 concurrent): ~50% success (100/200)
- **Tier 2** (12 concurrent): ~85% success (170/200)
- **Tier 3** (20 concurrent): ~95%+ success (190+/200) ‚úÖ

---

## ‚ö° Ottimizzazioni Avanzate

### 1. **Usa Modello Pi√π Veloce**

GPT-4o √® **3x pi√π veloce** di GPT-4-turbo:

```typescript
// app/lib/assistant-manager.ts
const assistant = await openai.beta.assistants.create({
  model: "gpt-4o",  // ‚Üê Invece di "gpt-4-turbo-preview"
  // ...
});
```

**Beneficio**: Meno tempo di elaborazione = pi√π throughput

### 2. **Connection Pooling**

OpenAI SDK usa http agent. Ottimizza:

```typescript
// app/openai.ts
import OpenAI from "openai";
import { Agent } from "https";

export const openai = new OpenAI({
  httpAgent: new Agent({
    keepAlive: true,
    maxSockets: 50,  // ‚Üê Pi√π socket = pi√π concurrent
  }),
});
```

### 3. **Caching Risposte Comuni**

Se molti utenti fanno domande simili:

```typescript
// app/lib/response-cache.ts
const cache = new Map<string, string>();

export function getCachedResponse(question: string): string | null {
  const normalized = question.toLowerCase().trim();
  return cache.get(normalized) || null;
}

export function setCachedResponse(question: string, response: string) {
  const normalized = question.toLowerCase().trim();
  cache.set(normalized, response);
}
```

---

## üö® Monitoraggio

### Dashboard Metriche

Aggiungi endpoint per metriche:

```typescript
// app/api/metrics/route.ts
import { openaiQueue } from "@/app/lib/openai-queue";

export async function GET() {
  const stats = openaiQueue.getStats();

  return Response.json({
    timestamp: new Date().toISOString(),
    queue: stats,
    health: {
      status: stats.isOverloaded ? "degraded" : "healthy",
      capacity: `${stats.queueSize + stats.processing}/${stats.maxConcurrent + stats.maxQueueSize}`,
      utilizationPercent: Math.round(
        ((stats.queueSize + stats.processing) / (stats.maxConcurrent + stats.maxQueueSize)) * 100
      ),
    },
  });
}
```

### Alert Automatici

```bash
# monitor-alerts.sh
while true; do
  UTIL=$(curl -s https://your-app.vercel.app/api/health | jq -r '.queue.utilization')

  if [ "$UTIL" -gt 80 ]; then
    echo "üö® ALERT: Queue utilization at $UTIL%"
    # Invia notifica (Slack, email, etc.)
  fi

  sleep 10
done
```

---

## ‚úÖ Checklist Finale

Prima di andare in produzione con 200 utenti:

- [ ] Verificato tier OpenAI (dovrebbe essere Tier 3+ per 200 utenti)
- [ ] Configurato MAX_CONCURRENT appropriato (20 per Tier 3)
- [ ] Configurato MAX_QUEUE_SIZE appropriato (250 per Tier 3)
- [ ] Ridotto MIN_REQUEST_DELAY (50ms per Tier 3)
- [ ] Deployato su Vercel
- [ ] Verificato `/api/health` mostra nuovi valori
- [ ] Testato con script test-200-users.sh
- [ ] Success rate >90%
- [ ] Monitorato OpenAI dashboard (usage <70% del limite)
- [ ] Setup monitoring continuo

---

## üÜò Troubleshooting

### "Ancora errori 429 con Tier 3"

**Causa**: MAX_CONCURRENT ancora troppo alto per il tuo uso reale.

**Soluzione**: Riduci gradualmente:
```typescript
private readonly MAX_CONCURRENT = 15; // Prova 15 invece di 20
```

### "Queue sempre piena (utilization >90%)"

**Causa**: Troppi utenti per la configurazione attuale.

**Soluzioni**:
1. Aumenta MAX_QUEUE_SIZE a 300
2. Aumenta MAX_CONCURRENT (se hai margine nel tier)
3. Upgrade tier OpenAI

### "Success rate solo 70% con 200 utenti"

**Causa**: Configurazione non abbastanza aggressiva.

**Soluzione**: Se hai Tier 3+:
```typescript
private readonly MAX_CONCURRENT = 25;
private readonly MAX_QUEUE_SIZE = 300;
```

---

## üí∞ Costi Stimati

| Tier | Spesa Minima | Costo/1K req | 200 utenti/giorno | Costo/mese |
|------|-------------|--------------|-------------------|------------|
| Tier 1 | $5 | $0.03 | ~6K req/giorno | ~$15-20 |
| Tier 2 | $50 | $0.03 | ~6K req/giorno | ~$15-20 |
| **Tier 3** | **$100** | **$0.03** | **~6K req/giorno** | **~$15-20** |

**Nota**: Il costo per request √® uguale. Paghi il tier per avere **limiti pi√π alti**, non per pagare meno per request.

---

## üéØ Raccomandazione Finale

Per **200 utenti simultanei**:

1. ‚úÖ **Upgrade a OpenAI Tier 3** ($100+ spesi)
2. ‚úÖ **Configura MAX_CONCURRENT = 20**
3. ‚úÖ **Configura MAX_QUEUE_SIZE = 250**
4. ‚úÖ **Configura MIN_REQUEST_DELAY = 50ms**
5. ‚úÖ **Testa con script di carico**
6. ‚úÖ **Monitora costantemente**

Con questa configurazione gestirai **270 utenti simultanei** con margine di sicurezza. üöÄ

---

**Ultima modifica**: 2025-10-23
**Versione**: 1.0.0
